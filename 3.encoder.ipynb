{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4650a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "\n",
    "class Embed(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = torch.nn.Parameter(torch.randn(1, 1, 1024))\n",
    "        self.body = torch.nn.Conv2d(3, 1024, 14, 14, 0)\n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, 1369 + 1, 1024))\n",
    "\n",
    "    def get_pos(self):\n",
    "        #[1, 1, 1024]\n",
    "        head = self.pos[:, :1]\n",
    "        #[1, 1369, 1024]\n",
    "        body = self.pos[:, 1:]\n",
    "\n",
    "        #[1, 1369, 1024] -> [1, 37, 37, 1024] -> [1, 1024, 37, 37]\n",
    "        body = body.reshape(1, 37, 37, 1024).permute(0, 3, 1, 2)\n",
    "\n",
    "        dtype = body.dtype\n",
    "\n",
    "        #0.9756756756756757\n",
    "        scale_factor = float((512 // 14 + 0.1) / 1369**0.5)\n",
    "\n",
    "        #[1, 1024, 37, 37] -> [1, 1024, 36, 36]\n",
    "        body = torch.nn.functional.interpolate(body.float(),\n",
    "                                               scale_factor=(scale_factor,\n",
    "                                                             scale_factor),\n",
    "                                               mode='bicubic',\n",
    "                                               align_corners=False)\n",
    "\n",
    "        #[1, 1024, 36, 36] -> [1, 36, 36, 1024] -> [1, 1296, 1024]\n",
    "        body = body.to(dtype=dtype).permute(0, 2, 3, 1).reshape(1, -1, 1024)\n",
    "\n",
    "        #[1, 1 + 1296, 1024] -> [1, 1297, 1024]\n",
    "        return torch.cat((head, body), dim=1)\n",
    "\n",
    "    def get_pos(self, size):\n",
    "        #[1, 1, 1024]\n",
    "        head = self.pos[:, :1]\n",
    "        #[1, 1369, 1024]\n",
    "        body = self.pos[:, 1:]\n",
    "\n",
    "        size = [(i // 14 + 0.1) / 37.0 for i in size]\n",
    "\n",
    "        body = body.reshape(1, 37, 37, 1024).permute(0, 3, 1, 2)\n",
    "        body = torch.nn.functional.interpolate(body.float(),\n",
    "                                               scale_factor=size,\n",
    "                                               mode='bicubic',\n",
    "                                               align_corners=False)\n",
    "        body = body.to(dtype=self.pos.dtype)\n",
    "\n",
    "        body = body.permute(0, 2, 3, 1).view(1, -1, 1024)\n",
    "\n",
    "        return torch.cat((head, body), dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = list(x.shape[2:])\n",
    "        x = self.body(x).flatten(2).transpose(1, 2)\n",
    "        x = torch.cat((self.head, x), dim=1)\n",
    "        return x + self.get_pos(size)\n",
    "\n",
    "\n",
    "class Atten(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q = torch.nn.Linear(1024, 1024)\n",
    "        self.k = torch.nn.Linear(1024, 1024)\n",
    "        self.v = torch.nn.Linear(1024, 1024)\n",
    "        self.out = torch.nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, dim, 1024]\n",
    "        dim = x.shape[1]\n",
    "\n",
    "        #维度不变\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        #[1, dim, 1024] -> [1, dim, 16, 64] -> [1, 16, dim, 64]\n",
    "        q = q.reshape(1, dim, 16, 64).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(1, dim, 16, 64).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(1, dim, 16, 64).permute(0, 2, 1, 3)\n",
    "\n",
    "        #[1, 16, dim, 64] * [1, 16, 64, dim] -> [1, 16, dim, dim]\n",
    "        atten = q.matmul(k.transpose(2, 3))\n",
    "\n",
    "        atten = atten / 64**0.5\n",
    "\n",
    "        atten = atten.softmax(dim=-1)\n",
    "\n",
    "        #[1, 16, dim, dim] * [1, 16, dim, 64] -> [1, 16, dim, 64]\n",
    "        atten = atten.matmul(v)\n",
    "\n",
    "        #[1, 16, dim, 64] -> [1, dim, 16, 64]\n",
    "        atten = atten.permute(0, 2, 1, 3)\n",
    "\n",
    "        #[1, dim, 16, 64] -> [1, dim, 1024]\n",
    "        atten = atten.reshape(1, dim, 1024)\n",
    "\n",
    "        return self.out(atten)\n",
    "\n",
    "\n",
    "class Layer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.atten = Atten()\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(1024, 4096),\n",
    "                                       torch.nn.GELU(),\n",
    "                                       torch.nn.Linear(4096, 1024))\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(1024, eps=1e-6)\n",
    "        self.norm2 = torch.nn.LayerNorm(1024, eps=1e-6)\n",
    "\n",
    "        self.scala1 = torch.nn.Parameter(torch.ones(1024))\n",
    "        self.scala2 = torch.nn.Parameter(torch.ones(1024))\n",
    "\n",
    "    def forward(self, x):\n",
    "        atten = self.atten(self.norm1(x)) * self.scala1\n",
    "\n",
    "        res = atten + x\n",
    "\n",
    "        out = self.mlp(self.norm2(res)) * self.scala2\n",
    "\n",
    "        return out + res\n",
    "\n",
    "\n",
    "class Encoder(PreTrainedModel):\n",
    "    config_class = PretrainedConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embed = Embed()\n",
    "        self.layers = torch.nn.ModuleList([Layer() for _ in range(24)])\n",
    "        self.norm = torch.nn.LayerNorm(1024, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        for i in self.layers:\n",
    "            x = i(x)\n",
    "\n",
    "        return self.norm(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
